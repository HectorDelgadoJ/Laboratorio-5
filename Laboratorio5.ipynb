{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOonMZXiCPhXq9BCUZOb0c0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HectorDelgadoJ/Laboratorio-5/blob/main/Laboratorio5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medidas de Desempeño en ML\n",
        "\n",
        "En esta práctica, implementaremos una serie de métricas utilizadas para evaluar el rendimiento de los modelos de clasificación binaria. Estas métricas son esenciales para comprender la eficacia de un modelo, especialmente cuando se enfrentan clases desbalanceadas. A través de un conjunto de datos de prueba, calcularemos la matriz de confusión y derivaremos medidas como precisión, recall, F1-score, entre otras. Además, implementaremos estas funciones desde cero en Python, sin el uso de bibliotecas especializadas, lo que nos permitirá comprender mejor los fundamentos de cada métrica y su aplicación en escenarios reales de clasificación."
      ],
      "metadata": {
        "id": "B0WZTIvL1eC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Programa, sin bibliotecas, las funciones para calcular las siguientes medidas de desempeño:\n",
        "1- Accuracy\n",
        "2- Error.\n"
      ],
      "metadata": {
        "id": "aWNvJVcv2azl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular Accuracy y Error\n",
        "def calculate_accuracy_error(y_true, y_pred):\n",
        "    correct_predictions = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
        "    total_predictions = len(y_true)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    error = 1 - accuracy\n",
        "    return accuracy, error"
      ],
      "metadata": {
        "id": "kcnrloWX2s2x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 3- Programa un método para calcular la matriz de confusión en datasets con dos clases y a partir de ésta calcula las siguientes medidas.\n",
        "* a)Precision\n",
        "* b)Recall\n",
        "* c)Positive Predictive Value\n",
        "* d)True Positive Rate\n",
        "* e)True Negative Rate\n",
        "* f)False Positive Rate\n",
        "* g)False Negative Rate\n",
        "* h)F1-Score"
      ],
      "metadata": {
        "id": "2vZmUvQF22Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular la matriz de confusión para un dataset binario\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    TP = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n",
        "    TN = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 0)\n",
        "    FP = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n",
        "    FN = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n",
        "    return TP, TN, FP, FN\n",
        "\n",
        "# Funciones para calcular las métricas de desempeño\n",
        "def calculate_metrics(TP, TN, FP, FN):\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    true_positive_rate = recall\n",
        "    true_negative_rate = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "    false_positive_rate = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "    false_negative_rate = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"Positive Predictive Value (PPV)\": precision,\n",
        "        \"True Positive Rate (TPR)\": true_positive_rate,\n",
        "        \"True Negative Rate (TNR)\": true_negative_rate,\n",
        "        \"False Positive Rate (FPR)\": false_positive_rate,\n",
        "        \"False Negative Rate (FNR)\": false_negative_rate,\n",
        "        \"F1-Score\": f1_score\n",
        "    }\n",
        "\n",
        "# Ejemplo de uso\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]  # Valores reales\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]  # Valores predichos\n",
        "\n",
        "# Calcular accuracy y error\n",
        "accuracy, error = calculate_accuracy_error(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Error:\", error)\n",
        "\n",
        "# Calcular matriz de confusión\n",
        "TP, TN, FP, FN = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calcular métricas de desempeño\n",
        "metrics = calculate_metrics(TP, TN, FP, FN)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "id": "qIUFvY-h4NID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Matriz de confusión: Una matriz de confusión organiza los resultados de predicción en categorías: Verdaderos Positivos (TP), Falsos Positivos (FP), Verdaderos Negativos (TN), y Falsos Negativos (FN).\n",
        "* Cálculo de métricas derivadas de la matriz de confusión:\n",
        "- a) Precision: TP/(TP+FP)\n",
        "- b) Recall o True Positive Rate: TP/(TP+FN)\n",
        "- c) Positive Predictive Value (PPV): Igual a Precision\n",
        "- d) True Negative Rate: TN/(TN+FP)\n",
        "- e) False Positive Rate: FP/(FP+TN)\n",
        "- f) False Negative Rate: FN/(FN+TP)\n",
        "- g) F1-Score: 2×(Precision×Recall)/(Precision+Recall)"
      ],
      "metadata": {
        "id": "-2Dqcghk4YsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parte II: Implementación con una biblioteca de Python (scikit-learn)\n",
        "En Python, la biblioteca scikit-learn facilita el cálculo de estas métricas. Aquí tienes cómo implementarlas usando scikit-learn:"
      ],
      "metadata": {
        "id": "Eab9BmLh6WzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Valores reales y predichos\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "# Accuracy y Error\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "error = 1 - accuracy\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Error:\", error)\n",
        "\n",
        "# Matriz de Confusión\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
        "\n",
        "# Métricas\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "\n",
        "# Calcular TNR, FPR, FNR usando la matriz de confusión\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "TNR = TN / (TN + FP)\n",
        "FPR = FP / (FP + TN)\n",
        "FNR = FN / (FN + TP)\n",
        "print(\"True Negative Rate (TNR):\", TNR)\n",
        "print(\"False Positive Rate (FPR):\", FPR)\n",
        "print(\"False Negative Rate (FNR):\", FNR)\n"
      ],
      "metadata": {
        "id": "3N9ys6g66eiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}